---
author: Gabriele Zarcone
date: 27 marzo 2018
title: Verifica e Convalida
---
---------------------------------------------

# Terminologia

### Convalida: 

> confronto del software con le idee e i bisogni dell'utente **informali** (espressi o inespressi). Controllare che si è fatto quello che voleva. 
>
> Nella convalida verifico l'usabilità e quando chiedo un feedback all'utente

### Verifica:

> confronto del sftware e dei suoi sottoprodotti con le specifiche **formali** scritte e raccolte dall'analista quando ha parlato con l'utente.
>
> Quando faccio Test automatico

Il confine tra verifica e convalida spesso non è così visibile. Spesso se alcune cose generiche sono facilmente interpretabili e quindi non posso verificarlo ma solo convalidarlo. (ES): *finisce in poco tempo*

Servono quindi delle specifiche formali per evitarlo.

# Errore (BUG)

qundo usiamo queste parole indichiamo spesso cose diverse. C'è una terminologia apposta.

### Malfunzionamento (Failure)

* funzionamento non corretto di un programma
* è legato al funzionamento del programma e **non** al suo codice

### Difetto o Anomalia (Fault)

è legato al codice ed è condizione necessaria (ma non sufficente) per il verificarsi di un malfunzionamento

NB se non ci sono anomalie non ci sono malfunzionamenti mentre può essere vero il contrario. Per avere un malfunzionamento ci deve esssere un anomalia da qualche parte.

Non vale il contrario perchè potrebbero esserrci due errori che si compensano. Non è percepibile come malfunzionamento.

### Errore (Error)

è la causa di un anomalia. 

in genere si tratta di errori umani:
    
* concettuale
* battitura
* scarsa conoscenza del linguaggio

## Ariane 5

### Malfunzionamento
dopo 40 secondi esplode il razzo Ariane 5. Dal punto di vista esterno è stato subito chiaro il problema (è esploso). E anche dalla sala controllo se ne erano accorti perchè uscivano un sacco di errori perchè creschava il sistema di riferimento inerziale. Sono loro che lo hanno fatto esplodere per sicurezza. Il malfunzionamento che sembrava a tutti in teoria (esplosione) non era quello ma un altro.

### Anomalia

un float veniva assegnato ad un int e andava in overflow. Da 64 bit a 16 bit. Il sistema tirava un eccezione che non veniva gestito. 

### Errore

il codice era scritto bene perchè non era fatto per l'Arian 5 ma per l'Arian 4. Quella riga calcolava la pendenza di decollo del razzo che per l'Arien 4 era molto più basso. Andava bene anche un int

Errore è stato riutilizzare un codice dando per scontato fosse funzionante (dato che aveva sempre funzionato) senza mai testare una situazione reale. 

### Considerazioni

E in più le eccezioni erano usate solo per i controlli HW e non SW. Il SW è considerato come componente secondario. 


Il software può creare problemi enormi anche per anomalie molto piccole. (ES) team inglese e americano che fanno progetto insieme e uno usa le miglia e l'altro i metri.

# Statico vs Dinamico

## Tecniche statiche
tutte le tecniche che lavorano suil codice senza eseguirlo.

la sto facendo su elementi del codice e non su elementi di stato di evoluzione del programma. Quindi la loro complessità dipende dalla dimensione del programma e non dall'evoluzione

* metodi formali
* analisi Data Flow
* Modelli Statistici

Sono molto più applicabili anche a programmi molto grossi. Perchè è qualcosa di sicuramente finito.

## Tecniche Dinamiche
tutte le tecniche che lavorano sul codice e lo devono eseguire.

Si guuarda il numero di stati in cui si può trovare il sistema che è enormemente più grande di quante righe di codice vengono scritte. La loro complessità è molto elevata per programmi grossi. 

* Testing
* Debugging

# Classificazione delle tecniche

cosa possiamo fare per trovare un problema. Voglio sapere se sono in grado di rispondere all domanda: Sono capace che il software è corretto al 100%?

Non è possibile! Servirebbe testare tutti gli input. MI servirebbe tanto tempo quanto la durata dell'universo.

Risolvo in 3 modi:

1. **Semplifico il processo:**

    Mi pongo una domanda più semplice e meno complessa: Sono corrette le funzionalità principali del mio programma?

2. **Innacuratezza pessimistica**:

    Altrimenti posso dire: Se non posso dimostrarti che è corretto dico che   non è corretto. Non voglio dire che è corretto se non sono sicuro quindi piuttosto dico che è scorretto

3. **Innaccuratezza Ottimistica**:

    è quella del testing. Faccio un po' di prove che tengono conto dei casi esemplari. Se passa quelli però non è detto che sia corretto, ma io dico che è corretto

Posso usare uno di questi tre metodi indipendentemente uno dall'altro. Devo però essere consapevole di quale sto usando.


# Prova formale

> sono tecniche che si prefiggono di provare l'assenza di *anomalie* (e quindi anche malfunzionamento) nel prodotto finale

es:

* ..
* ..

# Testing

> Tecniche di verifica che si prefiggono di rilevare *malfunzionamenti*
>
> o fornire fiducia del prodotto (test di accettazione)

NB il testing non è sinonimo di verifica. E' un tipo di verifica

es:

* **White Box** = scatola che mi permette di guardare dentro. Posso scegliere gli input, conoscendo il mio codice.
* **Black Box** = non conosco il mio codice. La scelta dell'input non posso farla partendo dal codice.
* **Gray Box** = conosco l'architettura sottostante (es. c'è un database. c'è un pattern MVC). Uso degli input che vanno a stimolare tutte le parti.

# Debugging 

> Tecnica che si prefigge di provare anomalie che causano malfunzionamento. Parto da un punto in cui so già che c'è un malfunzionamento e so qual'è e posso riprodurlo. 

Partendo da qui provo a risolvere l'errore.

es:

* riproduco li stati intermedi dell'esecuzione del programma. eli testo uno a uno (printf ovunque)
* Approccio incrementale: tolgo parte del codice fino a che non crasha più. così da scoprire qual'è la sezione di codice che ha il difetto

# Definizioni

## Correttezza Programma

considerando un generico progamma `P` come una funzione da un insieme di dati`D` (dominio) a un insieme di dati `R` (codominnio)

* `P(d)` indica l'esecuzione di P sul dato in ingresso d appartente a D

* il risultato P(d) è **corretto** se soddisfa le specifiche, altrimenti è scorretto.

* `ok(P,d)` indica la correttezza di P per il dato d

> P è corretto `se e solo se`  `per ogni d appart. D` ok(P,d)



## Test

Un Test `T` per un programma P èun sottinsieme del dominio. Deve essere finito anche se il Dominio è infinito

Un elemento del test è detto **caso del test**. Un test è un insieme di casi di test. E' un insieme di stimolazioni. 

* ESECUZIONE DEL TEST esecuzione del programma per tutti i casi di test `t` appartenenti al test T

ok(P,T) <-> ∀t∈T ok(P,t)

## Successo del test

`successo(T,P)`

un test ha *successo* quando **TROVA** gli errori prima ignoti

Se il test non ha successo il programma passa il test

## Test ideale

T è ideale su P se e solo se: ok(P, T) -> ok(P, D)

> cioè se la correttezza sui soli casi di test mi implica la correttezza su tutto il dominio

In generale è impossibile trovare un test ideale. L'unico test ideale che posso fare è se T è uguale a D. Ma è impossibile dal punto di vista del tempo

Non posso scegliere a caso i miei test. Devo prendere dei casi signoficativi, secondo un **criterio di scelta**

# Criterio

> prendere un sottominio come test seguendo un certo ragionamento

## Qualità Criteri

### Affidabile

> un criterio è affidabile se dati 2 test qualsiasi T1 e T2 entrambi selezionati secondo un criterio C se quando ha successo uno ha **sempre** successo anche l'altro

Come faccio a definire un criterio affidabile? Non esiste un criterio affidabile perchp nonn ne posso essere certo. L'unico test affidabile è quello che ha un solo test (un solo gruppo di stimolazioni)

### Valido

> un criterio è **valido** se qual'ora il programma P è non corretto allora almeno *uno* dei test deve avere successo 

0:21:21

Un criterio potrebbe essere valido ma non affidabile o viceversa. Ma io ne vorrei uno sia affidabile che valido. Il test posso farlo però sia affidabile che valido solo se conosco come è il programma. Voglio però che siano validi e affidabile per qualsiasi programma

# Teorema

se hai un test valido e affidabile e il tuo test falliscw allora il tuo programma funziona

Perchp 

* essendo valido se c'era un errore almeno uno dei test lo deve trovare (valido)
* tutti i test danno lo stesso risultato (affidabile)

allora mi bast averificare un solo test per vedere se i programma è corretto: tutti gli altri mi darebbero lo stesso risultato quindi se almeno uno ha successo allora tutti hanno successo

Se esistesse un criterio aff e valido contemporaneamente selezionerebbe solo test ideali ===> non può esistere questo criterio.

## Criterio a priori valido

Dominio infinito

Sottoinsiemi del dominio finiti

Dovrei essere capace di coprire con test finiti un insieme infinito perchè non solo con quale valore del dominio, in quale parte del dominio, c'è il problema

E' possibile solo con un criterio che mi dia infiniti test. Ma io voglio un criterio proprio però per evitare di avere infinite cose da controllare, ma solo un caso significativo

# Problema dell'oracolo

Devo sapere qual'è il risultato esatto che mi deve fare il test. Chi mi calcola il risultato esatto? Devo farmelo a mano io?

# Approccio White Box 

gli errori si annidano negli angoli
: nelle zone del programma che non ho considerato, nei casi speciali e particolari e meno comuni del mio dominio. 

Cioè Errori logici e assunzioni corrette sono inversamente proporzionali alla probabilita` che un certo cammino venga eseguito

In più spesso quello che ci sembra un cammino molto raro e particolare spesso è in realtà un percorso molto usato.

Gli errori di battitura stupidi e casuali possono creare dei problemi enormi. 

L'errore può essere ovunque. 

Perchè un caso di test trovi un malfunzionamento devono accadere tre cose:

* il test deve stimolare l'anomalia. 
*  l'eseuczione di quel comando deve essere tale che il difetto venga evidenziato arrivando ad uno styato inconsistente
*  l'inconsistenza si deve propagare fino a diventare evidente e visibile 

# Criterio di Copertura

Ad ogni criterio è possibile associare una misura di copertura. Cioè coprire totalmente un criterio = lo soddisfo in pieno oppure coprirlo ad una certa percentuale.

Posso coprire un criterio anche solo all'80% 

Posso così decidere qunado fermare il test. Magari non controllo tutto ma riesco a controllare una buona percentuale

## [1] Copertura di tutti i comandi 

Criterii di copertura che per ogni comando del mio comando deve esserci almeno un caso di test che vada a stimolarmi quel comando. Se un comando non fosse stimolato non saprò mai se c'è un errore in quel comando dato che non lo copre. Non c'è copertura su quel comando. Per vedere un errore devo per forza eseguire i comandi per cheè non conosco il programma.

La percentuale diventa quindi comandi eseguiti / comandi eseguibili

es. copre i 5/7 del programma

comandi eseguibili (=comandi non raggiungilbili ===> non c'è bisgno di coprirli quando faccio il test)

### Grafo di controllo

> * ogni istruzione è un nodo 
> * le frecce mi portano tra le varie istreuzioni. 
> 
> * Ramificazioni = if

Criterio di copertura dei comandi è intrensecamente valido perchè sono inifiniti e tra loro trova sicuramente l'errrore. Non è affidabile ma è valido

Coprire i comandi vuol dire coprire i nodi

Ma potrei volere ocprire anche l'arcio percheè sono i diversi modi n cui arrivo ad un istruzione. Magari arrivare da un istruzione o da un altra mi porta o meno ad un problema. Voglio coprire tutti gli archi e quindi tutti i modi in cui ci psosso arrivare. Copro quindi tutte le decisioni. Ogni volta che c'è una decisione e quindi una ramificazione controllo tutte le decisioni

## [2] Copertura delle decisioni

> Un test soddisfa il criterio di copertura delle decisioni se e solo se ogni arco del grafo di controllo del programma è percorso almeno una volta in corrispondenza di almeno un dato di test t contenuto in T

1:00:19




-------------------------------------------------------------------